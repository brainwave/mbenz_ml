{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCreated on Tuesday Nov 30 2021\\n\\n@author: brahmprakash.mishra\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "\"\"\"\n",
    "Created on Tuesday Nov 30 2021\n",
    "\n",
    "@author: brahmprakash.mishra\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## This script contains four independent models, the stacking part: Use lgb, gbdt and Lasso as \n",
    "## Level 1 models. Meta model is xgb. This may count for 25% of the final result.\n",
    "## Then, we use a single xgb model forked from this kernel : https://www.kaggle.com/hakeem/stacked-then-averaged-models-0-5697\n",
    "## Only take single XGB.\n",
    "## Then, finally replace some values with LB Probing Values. I am not sure whether this is a good idea, just try\n",
    "\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import FastICA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "#print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial training dataset shape (4209, 378)\n",
      "testing dataset shape (4209, 377)\n",
      "training data shape after dropping after dropping target: (4209, 377)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#change variable names so we don't inadvertantnly use any variables from other solutions\n",
    "trainingData = pd.read_csv('dataset/train.csv')\n",
    "testingData = pd.read_csv('dataset/test.csv')\n",
    "\n",
    "yTrain=trainingData['y']\n",
    "Xtrain=trainingData.drop(['y'], axis = 1)\n",
    "yTest=np.zeros(trainingData.shape[0])\n",
    "Xtest = testingData\n",
    "\n",
    "print('initial training dataset shape',trainingData.shape)\n",
    "print('testing dataset shape',Xtest.shape)\n",
    "print('training data shape after dropping after dropping target:',Xtrain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that train and test conform to our analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['X0', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X8'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#get list of categorical columns\n",
    "categorical_columns = trainingData.select_dtypes(include = 'object').columns\n",
    "print(categorical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in categorical_columns:\n",
    "    lbl = LabelEncoder()\n",
    "    lbl.fit(list(Xtrain[col].values) + list(Xtest[col].values))\n",
    "    Xtrain[col] = lbl.transform(list(Xtrain[col].values))\n",
    "    Xtest[col] = lbl.transform(list(Xtest[col].values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data\n",
      "    X0  X1  X2  X3  X4  X5  X6  X8\n",
      "0  37  23  20   0   3  27   9  14\n",
      "1  37  21  22   4   3  31  11  14\n",
      "2  24  24  38   2   3  30   9  23\n",
      "3  24  21  38   5   3  30  11   4\n",
      "4  24  23  38   5   3  14   3  13\n",
      "testing data\n",
      "     X0  X1  X2  X3  X4  X5  X6  X8\n",
      "0  24  23  38   5   3  26   0  22\n",
      "1  46   3   9   0   3   9   6  24\n",
      "2  24  23  19   5   3   0   9   9\n",
      "3  24  13  38   5   3  32  11  13\n",
      "4  49  20  19   2   3  31   8  12\n"
     ]
    }
   ],
   "source": [
    "#ensure both datasets are encoded\n",
    "print('Training data\\n', Xtrain[categorical_columns].head())\n",
    "print('Testing data\\n ',Xtest[categorical_columns].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert columns X1 to X8 to one-shot encoding\n",
    "combine=pd.concat([Xtrain,Xtest])\n",
    "for column in categorical_columns[1:]:\n",
    "    temp=pd.get_dummies(pd.Series(combine[column]))\n",
    "    combine=pd.concat([combine,temp],axis=1)\n",
    "    combine= combine.drop([column], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some useful functions\n",
    "\n",
    "train=combine[:train.shape[0]]\n",
    "test=combine[train.shape[0]:] \n",
    "\n",
    "\n",
    "def df_column_uniquify(df):\n",
    "    df_columns = df.columns\n",
    "    new_columns = []\n",
    "    for item in df_columns:\n",
    "        counter = 0\n",
    "        newitem = item\n",
    "        while newitem in new_columns:\n",
    "            counter += 1\n",
    "            newitem = \"{}_{}\".format(item, counter)\n",
    "        new_columns.append(newitem)\n",
    "    df.columns = new_columns\n",
    "    return df\n",
    "\n",
    "train = df_column_uniquify(train)  \n",
    "test = df_column_uniquify(test)   \n",
    "train['y']=y\n",
    "\n",
    "def get_additional_features(train,test,magic=False,ID=False):\n",
    "    col = list(test.columns)\n",
    "    if ID!=True:\n",
    "        col.remove('ID')\n",
    "    n_comp = 12\n",
    "    # tSVD\n",
    "    tsvd = TruncatedSVD(n_components=n_comp, random_state=420)\n",
    "    tsvd_results_train = tsvd.fit_transform(train[col])\n",
    "    tsvd_results_test = tsvd.transform(test[col])\n",
    "    # PCA\n",
    "    pca = PCA(n_components=n_comp, random_state=420)\n",
    "    pca2_results_train = pca.fit_transform(train[col])\n",
    "    pca2_results_test = pca.transform(test[col])\n",
    "    # ICA\n",
    "    ica = FastICA(n_components=n_comp, random_state=420)\n",
    "    ica2_results_train = ica.fit_transform(train[col])\n",
    "    ica2_results_test = ica.transform(test[col])\n",
    "    # GRP\n",
    "    grp = GaussianRandomProjection(n_components=n_comp, eps=0.1, random_state=420)\n",
    "    grp_results_train = grp.fit_transform(train[col])\n",
    "    grp_results_test = grp.transform(test[col])\n",
    "    # SRP\n",
    "    srp = SparseRandomProjection(n_components=n_comp, dense_output=True, random_state=420)\n",
    "    srp_results_train = srp.fit_transform(train[col])\n",
    "    srp_results_test = srp.transform(test[col])\n",
    "    for i in range(1, n_comp + 1):\n",
    "        train['tsvd_' + str(i)] = tsvd_results_train[:, i - 1]\n",
    "        test['tsvd_' + str(i)] = tsvd_results_test[:, i - 1]\n",
    "        train['pca_' + str(i)] = pca2_results_train[:, i - 1]\n",
    "        test['pca_' + str(i)] = pca2_results_test[:, i - 1]\n",
    "        train['ica_' + str(i)] = ica2_results_train[:, i - 1]\n",
    "        test['ica_' + str(i)] = ica2_results_test[:, i - 1]\n",
    "        train['grp_' + str(i)] = grp_results_train[:, i - 1]\n",
    "        test['grp_' + str(i)] = grp_results_test[:, i - 1]\n",
    "        train['srp_' + str(i)] = srp_results_train[:, i - 1]\n",
    "        test['srp_' + str(i)] = srp_results_test[:, i - 1]\n",
    "    if magic==True:\n",
    "        magic_mat = train[['ID','X0','y']]\n",
    "        magic_mat = magic_mat.groupby(['X0'])['y'].mean()\n",
    "        magic_mat = pd.DataFrame({'X0':magic_mat.index,'magic':list(magic_mat)})\n",
    "        mean_magic = magic_mat['magic'].mean()\n",
    "        train = train.merge(magic_mat,on='X0',how='left')\n",
    "        test = test.merge(magic_mat,on='X0',how = 'left')\n",
    "        test['magic'] = test['magic'].fillna(mean_magic)\n",
    "    return train,test\n",
    "\n",
    "## Preparing stacking functions. Each one takes the out of bag values as the Input\n",
    "\n",
    "## xgb will not be used in this case, but still post it here.\n",
    "def get_xgb_stack_data(params,rounds,train,col,label,test):\n",
    "    ID = []\n",
    "    train = train.reset_index(drop=True)\n",
    "    kf = KFold(n_splits=5,shuffle=False)\n",
    "    i=0\n",
    "    R2_Score = []\n",
    "    RMSE = []\n",
    "    for train_index, test_index in kf.split(train):\n",
    "        print(\"Training \"+str(i+1)+' Fold')\n",
    "        X_train, X_test = train.iloc[train_index,:], train.iloc[test_index,:]\n",
    "        y_train, y_test = label.iloc[train_index],label.iloc[test_index]\n",
    "        dtrain = xgb.DMatrix(X_train[col],y_train)\n",
    "        dtest = xgb.DMatrix(X_test[col])\n",
    "        model = xgb.train(params,dtrain,num_boost_round=rounds)\n",
    "        pred = model.predict(dtest)\n",
    "        X_test['label'] = list(y_test)\n",
    "        X_test['predicted'] = pred\n",
    "        r2 = r2_score(y_test,pred)\n",
    "        rmse = MSE(y_test,pred)**0.5\n",
    "        print('R2 Scored of Fold '+str(i+1)+' is '+str(r2))\n",
    "        R2_Score.append(r2)\n",
    "        RMSE.append(rmse)\n",
    "        print('RMSE of Fold '+str(i+1)+' is '+str(rmse))\n",
    "        ID.append(X_test['ID'])\n",
    "        if i==0:\n",
    "            Final = X_test\n",
    "        else:\n",
    "            Final = Final.append(X_test,ignore_index=True)\n",
    "        i+=1\n",
    "    dtrain_ = xgb.DMatrix(train[col],label)\n",
    "    dtest_ = xgb.DMatrix(test[col])\n",
    "    print('Start Training')\n",
    "    model_ = xgb.train(params,dtrain_,num_boost_round=rounds)\n",
    "    Final_pred = model_.predict(dtest_)\n",
    "    Final_pred = pd.DataFrame({'ID':test['ID'],'y':Final_pred})\n",
    "    print('Calculating In-Bag R2 Score')\n",
    "    print(r2_score(dtrain_.get_label(), model.predict(dtrain_)))\n",
    "    print('Calculating Out-Bag R2 Score')\n",
    "    print(np.mean(R2_Score))\n",
    "    print('Calculating In-Bag RMSE')\n",
    "    print(MSE(dtrain_.get_label(), model.predict(dtrain_))**0.5)\n",
    "    print('Calculating Out-Bag RMSE')\n",
    "    print(np.mean(RMSE))\n",
    "    return Final,Final_pred\n",
    "\n",
    "\n",
    "def get_lgb_stack_data(params,rounds,train,col,label,test):\n",
    "    ID = []\n",
    "    train = train.reset_index(drop=True)\n",
    "    kf = KFold(n_splits=5,shuffle=False)\n",
    "    i=0\n",
    "    R2_Score = []\n",
    "    RMSE = []\n",
    "    for train_index, test_index in kf.split(train):\n",
    "        print(\"Training \"+str(i+1)+' Fold')\n",
    "        X_train, X_test = train.iloc[train_index,:], train.iloc[test_index,:]\n",
    "        y_train, y_test = label.iloc[train_index],label.iloc[test_index]\n",
    "        train_lgb=lgb.Dataset(X_train[col],y_train)\n",
    "        model = lgb.train(params,train_lgb,num_boost_round=rounds)\n",
    "        pred = model.predict(X_test[col])\n",
    "        X_test['label'] = list(y_test)\n",
    "        X_test['predicted'] = pred\n",
    "        r2 = r2_score(y_test,pred)\n",
    "        rmse = MSE(y_test,pred)**0.5\n",
    "        print('R2 Scored of Fold '+str(i+1)+' is '+str(r2))\n",
    "        R2_Score.append(r2)\n",
    "        RMSE.append(rmse)\n",
    "        print('RMSE of Fold '+str(i+1)+' is '+str(rmse))\n",
    "        ID.append(X_test['ID'])\n",
    "        if i==0:\n",
    "            Final = X_test\n",
    "        else:\n",
    "            Final = Final.append(X_test,ignore_index=True)\n",
    "        i+=1\n",
    "    lgb_train_ = lgb.Dataset(train[col],label)\n",
    "    print('Start Training')\n",
    "    model_ = lgb.train(params,lgb_train_,num_boost_round=rounds)\n",
    "    Final_pred = model_.predict(test[col])\n",
    "    Final_pred = pd.DataFrame({'ID':test['ID'],'y':Final_pred})\n",
    "    print('Calculating In-Bag R2 Score')\n",
    "    print(r2_score(label, model.predict(train[col])))\n",
    "    print('Calculating Out-Bag R2 Score')\n",
    "    print(np.mean(R2_Score))\n",
    "    print('Calculating In-Bag RMSE')\n",
    "    print(MSE(label, model.predict(train[col]))**0.5)\n",
    "    print('Calculating Out-Bag RMSE')\n",
    "    print(np.mean(RMSE))\n",
    "    return Final,Final_pred\n",
    "\n",
    "\n",
    "\n",
    "def get_sklearn_stack_data(model,train,col,label,test):\n",
    "    ID = []\n",
    "    R2_Score = []\n",
    "    RMSE = []\n",
    "    train = train.reset_index(drop=True)\n",
    "    kf = KFold(n_splits=5,shuffle=False)\n",
    "    i=0\n",
    "    for train_index, test_index in kf.split(train):\n",
    "        print(\"Training \"+str(i+1)+' Fold')\n",
    "        X_train, X_test = train.iloc[train_index,:], train.iloc[test_index,:]\n",
    "        y_train, y_test = label.iloc[train_index],label.iloc[test_index]\n",
    "        model.fit(X_train[col],y_train)\n",
    "        pred = model.predict(X_test[col])\n",
    "        X_test['label'] = list(y_test)\n",
    "        X_test['predicted'] = pred\n",
    "        r2 = r2_score(y_test,pred)\n",
    "        rmse = MSE(y_test,pred)**0.5\n",
    "        print('R2 Scored of Fold '+str(i+1)+' is '+str(r2))\n",
    "        R2_Score.append(r2)\n",
    "        RMSE.append(rmse)\n",
    "        print('RMSE of Fold '+str(i+1)+' is '+str(rmse))\n",
    "        ID.append(X_test['ID'])\n",
    "        if i==0:\n",
    "            Final = X_test\n",
    "        else:\n",
    "            Final = Final.append(X_test,ignore_index=True)\n",
    "        i+=1\n",
    "    print('Start Training')\n",
    "    model.fit(train[col],label)\n",
    "    Final_pred = model.predict(test[col])\n",
    "    Final_pred = pd.DataFrame({'ID':test['ID'],'y':Final_pred})\n",
    "    print('Calculating In-Bag R2 Score')\n",
    "    print(r2_score(label, model.predict(train[col])))\n",
    "    print('Calculating Out-Bag R2 Score')\n",
    "    print(np.mean(R2_Score))\n",
    "    print('Calculating In-Bag RMSE')\n",
    "    print(MSE(label, model.predict(train[col]))**0.5)\n",
    "    print('Calculating Out-Bag RMSE')\n",
    "    print(np.mean(RMSE))\n",
    "    return Final,Final_pred\n",
    "    \n",
    "## Prepare output of level 1.\n",
    "\n",
    "## Prepare data\n",
    "\n",
    "train_,test_ = get_additional_features(train,test,magic=True)\n",
    "train_ = train_.sample(frac=1,random_state=420)\n",
    "col = list(test.columns)\n",
    "## Input 1: GBDT\n",
    "\n",
    "gb1 = GradientBoostingRegressor(n_estimators=1000,max_features=0.95,learning_rate=0.005,max_depth=4)\n",
    "gb1_train,gb1_test = get_sklearn_stack_data(gb1,train_,col,train_['y'],test_)\n",
    "\n",
    "## Input2: Lasso\n",
    "las1 = Lasso(alpha=5,random_state=42)\n",
    "las1_train,las1_test = get_sklearn_stack_data(las1,train_,col,train_['y'],test_)\n",
    "\n",
    "## Input 3: LGB\n",
    "params = {\n",
    "            'objective': 'regression',\n",
    "            'metric': 'rmse',\n",
    "            'boosting': 'gbdt',\n",
    "            'learning_rate': 0.0045 , #small learn rate, large number of iterations\n",
    "            'verbose': 0,\n",
    "            'num_iterations': 500,\n",
    "            'bagging_fraction': 0.95,\n",
    "            'bagging_freq': 1,\n",
    "            'bagging_seed': 42,\n",
    "            'feature_fraction': 0.95,\n",
    "            'feature_fraction_seed': 42,\n",
    "            'max_bin': 100,\n",
    "            'max_depth': 3,\n",
    "            'num_rounds': 800\n",
    "        }\n",
    "lgb_train, lgb_test = get_lgb_stack_data(params,800,train_,col,train_['y'],test_)\n",
    "\n",
    "## Stacking By xgb\n",
    "\n",
    "stack_train = gb1_train[['label','predicted']]\n",
    "stack_train.columns=[['label','gbdt']]\n",
    "stack_train['lgb']=lgb_train['predicted']\n",
    "stack_train['las'] = las1_train['predicted']\n",
    "\n",
    "stack_test = gb1_test[['ID','y']]\n",
    "stack_test.columns=[['ID','gbdt']]\n",
    "stack_test['lgb']=lgb_test['y']\n",
    "stack_test['las'] = las1_test['y']\n",
    "del stack_test['ID']\n",
    "\n",
    "## Meta Model: xgb\n",
    "\n",
    "y_mean = np.mean(train.y)\n",
    "\n",
    "col = list(stack_test.columns)\n",
    "\n",
    "params = {\n",
    "    'eta': 0.005,\n",
    "    'max_depth': 2,\n",
    "    'objective': 'reg:linear',\n",
    "    'eval_metric': 'rmse',\n",
    "    'base_score': y_mean, # base prediction = mean(target)\n",
    "    'silent': 1\n",
    "}\n",
    "\n",
    "dtrain = xgb.DMatrix(stack_train[col], stack_train['label'])\n",
    "dtest = xgb.DMatrix(stack_test[col])\n",
    "\n",
    "#xgb_cvalid = xgb.cv(params, dtrain, num_boost_round=2000, early_stopping_rounds=20,\n",
    " #   verbose_eval=50, show_stdv=True,seed=42)\n",
    "#xgb_cvalid[['train-rmse-mean', 'test-rmse-mean']].plot()\n",
    "#print('Performance does not improve from '+str(len(xgb_cvalid))+' rounds')\n",
    "\n",
    "model = xgb.train(params,dtrain,num_boost_round =900)\n",
    "pred_1 = model.predict(dtest)\n",
    "\n",
    "\n",
    "## Original XGB In Popular Kernel\n",
    "\n",
    "\n",
    "\n",
    "train_,test_ = get_additional_features(train,test,ID=True)\n",
    "\n",
    "xgb_params = {\n",
    "        'n_trees': 520, \n",
    "        'eta': 0.0045,\n",
    "        'max_depth': 4,\n",
    "        'subsample': 0.93,\n",
    "        'objective': 'reg:linear',\n",
    "        'eval_metric': 'rmse',\n",
    "        'base_score': y_mean, # base prediction = mean(target)\n",
    "        'silent': True,\n",
    "        'seed': 42,\n",
    "    }\n",
    "dtrain = xgb.DMatrix(train_.drop('y', axis=1), train_.y)\n",
    "dtest = xgb.DMatrix(test_)\n",
    "    \n",
    "num_boost_rounds = 1250\n",
    "model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=num_boost_rounds)\n",
    "y_pred = model.predict(dtest)\n",
    "\n",
    "## Average Two Solutions\n",
    "\n",
    "Average = 0.70*y_pred + 0.30*pred_1\n",
    "\n",
    "sub = pd.DataFrame({'ID':test['ID'],'y':Average})\n",
    "\n",
    "## LB Prob Values \n",
    "\n",
    "## I forget whose credit should be given, Please help me to find him/her!!\n",
    "\n",
    "leaks = {\n",
    "    1:71.34112,\n",
    "    12:109.30903,\n",
    "    23:115.21953,\n",
    "    28:92.00675,\n",
    "    42:87.73572,\n",
    "    43:129.79876,\n",
    "    45:99.55671,\n",
    "    57:116.02167,\n",
    "    3977:132.08556,\n",
    "    88:90.33211,\n",
    "    89:130.55165,\n",
    "    93:105.79792,\n",
    "    94:103.04672,\n",
    "    1001:111.65212,\n",
    "    104:92.37968,\n",
    "    72:110.54742,\n",
    "    78:125.28849,\n",
    "    105:108.5069,\n",
    "    110:83.31692,\n",
    "    1004:91.472,\n",
    "    1008:106.71967,\n",
    "    1009:108.21841,\n",
    "    973:106.76189,\n",
    "    8002:95.84858,\n",
    "    8007:87.44019,\n",
    "    1644:99.14157,\n",
    "    337:101.23135,\n",
    "    253:115.93724,\n",
    "    8416:96.84773,\n",
    "    259:93.33662,\n",
    "    262:75.35182,\n",
    "    1652:89.77625\n",
    "    }\n",
    "sub['y'] = sub.apply(lambda r: leaks[int(r['ID'])] if int(r['ID']) in leaks else r['y'], axis=1)\n",
    "\n",
    "sub.to_csv('subXgb_Stack_Stack_No_ID_with_onehot.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ecfe3e0ac1b474fc1dfd44304c2aca7aa2cab0bc3d496764b4f9dd6feab8a52f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('directml': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
